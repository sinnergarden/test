{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Using User Behaviour [Supervised Learning]\n",
    "\n",
    "- Dataset: Fraud User & other normal user in user profile\n",
    "    - feature: User Behaviour Features\n",
    "    - label: fraud types\n",
    "    \n",
    "- Method: multiclass classification (supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:48:12.489918Z",
     "start_time": "2019-11-08T09:48:12.094947Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import base64\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:48:12.506627Z",
     "start_time": "2019-11-08T09:48:12.494216Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_spark_session(**kwargs):\n",
    "    \"\"\"Initializing Spark context.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kwargs : dict\n",
    "        Variable number of keyword arguments to initialize the SparkContext\n",
    "        object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "     : SparkContext object\n",
    "\n",
    "    \"\"\"\n",
    "    conf = SparkConf() \\\n",
    "        .setAppName(kwargs.get('app_name', 'test')) \\\n",
    "        .set(\"spark.executor.memory\", kwargs.get('executor_memory', '20g')) \\\n",
    "        .set(\"spark.driver.memory\", kwargs.get('driver_memory', '30g')) \\\n",
    "        .set(\"spark.driver.maxResultSize\",\n",
    "             kwargs.get('max_result_size', '100g')) \\\n",
    "        .set(\"spark.executor.instances\", kwargs.get('num_executors', '100')) \\\n",
    "        .set(\"spark.executor.cores\", kwargs.get('num_cores', '4')) \\\n",
    "        .set(\"spark.sql.crossJoin.enabled\", True)  \\\n",
    "        .set(\"spark.cores.max\", kwargs.get('cores_max', '1000'))    \\\n",
    "        .set(\"spark.network.timeout\", kwargs.get('timeout', '3600s')) \\\n",
    "        .set(\"spark.executor.heartbeatInterval\", kwargs.get('heartbeat', '3500s')) \\\n",
    "        .set(\"spark.sql.shuffle.partitions\", kwargs.get('num_partitions', '4000')) \\\n",
    "        .set(\"spark.yarn.queue\", kwargs.get('spark_yarn_queue', 'ds-critical'))\n",
    "\n",
    "    spark_session = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()\n",
    "    \n",
    "    return spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:48:35.680001Z",
     "start_time": "2019-11-08T09:48:12.525966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_name = \"LiuMing\" # put your name here\n",
    "spark_appname = \"pyspark_Supervised_Learning_{}\".format(your_name)\n",
    "\n",
    "# spark conf parameter\n",
    "executor_memory = '10g'\n",
    "driver_memory = '30g'\n",
    "num_executors = '100'\n",
    "num_partitions = '3000'\n",
    "\n",
    "# get the spark object\n",
    "spark = get_spark_session(app_name=spark_appname,\n",
    "                         executor_memory=executor_memory,\n",
    "                         driver_memory=driver_memory,\n",
    "                         num_executors=num_executors,\n",
    "                         num_partitions=num_partitions)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark.sql('use shopee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Fraud and User Behaviour Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:48:36.582993Z",
     "start_time": "2019-11-08T09:48:35.684969Z"
    }
   },
   "outputs": [],
   "source": [
    "Fraud_query = \"\"\"\n",
    "    select *\n",
    "    from shopee_fraud_backend_id_db__fraud_user_tag_tab\n",
    "    where status=1\n",
    "\"\"\"\n",
    "fraud_user_df = spark.sql(Fraud_query)\n",
    "fraud_user_df = fraud_user_df.withColumn('date', F.from_unixtime('mtime', 'YYYY-MM-dd'))\n",
    "fraud_user_df = fraud_user_df\\\n",
    "                .filter(fraud_user_df.date.between('2019-07-01','2019-10-01'))\\\n",
    "                .orderBy(['date','tagid'])\n",
    "\n",
    "Fraud_tag_query = \"\"\"\n",
    "    select id as taggid, name\n",
    "    from shopee_backend_id_db__fraud_tag_tab\n",
    "    where severity = 1 and status = 1\n",
    "\"\"\"\n",
    "fraud_tag_df = spark.sql(Fraud_tag_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:49:09.310380Z",
     "start_time": "2019-11-08T09:48:36.588591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-----------+\n",
      "|name                                        |total_count|\n",
      "+--------------------------------------------+-----------+\n",
      "|DP Voucher                                  |308191     |\n",
      "|Voucher                                     |87541      |\n",
      "|Free Shipping                               |55405      |\n",
      "|Scam - Potential Scammer                    |50497      |\n",
      "|Duplicate listing                           |49715      |\n",
      "|Order Brushing                              |40388      |\n",
      "|Welcome Package                             |12069      |\n",
      "|Promotion T&C                               |12045      |\n",
      "|Prohibited listing                          |5149       |\n",
      "|SPL Overdue                                 |2742       |\n",
      "|Coin fraud                                  |2560       |\n",
      "|Use of emulator/simulator                   |2488       |\n",
      "|CB inactive seller                          |1551       |\n",
      "|Advertisement/poaching                      |764        |\n",
      "|Scam - Confirmed Scammer                    |688        |\n",
      "|Fake order without economic benefits to user|608        |\n",
      "|CC chargeback                               |456        |\n",
      "|Fast upload                                 |291        |\n",
      "|Slash Price                                 |282        |\n",
      "|Unauthorised Use of Personal / Credit Info  |146        |\n",
      "|Account Hacking - Victim                    |111        |\n",
      "|Account Hacking - Hacker                    |87         |\n",
      "|Counterfeit/copyrights                      |40         |\n",
      "|Bulk Registration                           |31         |\n",
      "|Rebates                                     |28         |\n",
      "|Competitor Link                             |25         |\n",
      "|Spam: keyword                               |9          |\n",
      "|CC points                                   |7          |\n",
      "|Seller Penalty                              |1          |\n",
      "|Offensive chat                              |1          |\n",
      "+--------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_df = fraud_user_df.join(fraud_tag_df, fraud_tag_df.taggid == fraud_user_df.tagid)\n",
    "fraud_category_df = fraud_df.groupBy('name').agg(F.count('userid').alias('total_count'))\\\n",
    "                    .orderBy('total_count', ascending=False)\n",
    "\n",
    "fraud_category_df.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:49:29.514640Z",
     "start_time": "2019-11-08T09:49:09.314637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userid: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "633916\n"
     ]
    }
   ],
   "source": [
    "fraud_df = fraud_df.selectExpr('userid', 'name')\n",
    "fraud_df.printSchema()\n",
    "print(fraud_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:49:29.734096Z",
     "start_time": "2019-11-08T09:49:29.520069Z"
    }
   },
   "outputs": [],
   "source": [
    "Normal_User_Query = \"\"\"\n",
    "    select userid\n",
    "    from user_profile\n",
    "    where last_login > '2019-07-01'\n",
    "\"\"\"\n",
    "all_user_df = spark.sql(Normal_User_Query)\n",
    "\n",
    "normal_user_df = all_user_df.join(fraud_df, ['userid'], how='left_anti')\n",
    "normal_user_df = normal_user_df.withColumn('name', F.lit('Normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:49:29.759688Z",
     "start_time": "2019-11-08T09:49:29.739734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userid: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df = normal_user_df.union(fraud_df)\n",
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:50:45.373681Z",
     "start_time": "2019-11-08T09:49:29.763903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103332168\n"
     ]
    }
   ],
   "source": [
    "print(user_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Behaviour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:50:45.393682Z",
     "start_time": "2019-11-08T09:50:45.380963Z"
    }
   },
   "outputs": [],
   "source": [
    "Behaviour_Feature_Query = \"\"\"\n",
    "    select *\n",
    "    from shopee_ds.regds_kg_user_behaviour_feature\n",
    "    where grass_region='ID'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:50:45.524252Z",
     "start_time": "2019-11-08T09:50:45.399166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: long (nullable = true)\n",
      " |-- total_active_time_in_last_30d: double (nullable = true)\n",
      " |-- total_session_count_in_last_30d: long (nullable = true)\n",
      " |-- total_ops_count_in_last_30d: long (nullable = true)\n",
      " |-- active_days_in_last_30d: string (nullable = true)\n",
      " |-- last_active_day: date (nullable = true)\n",
      " |-- hour_normalise: string (nullable = true)\n",
      " |-- action_normalise: string (nullable = true)\n",
      " |-- op_interval: string (nullable = true)\n",
      " |-- markov: string (nullable = true)\n",
      " |-- grass_region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_df = spark.sql(Behaviour_Feature_Query)\n",
    "feature_df.printSchema()\n",
    "df = user_df.join(feature_df, user_df.userid == feature_df.uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters\n",
    "\n",
    "We only select top 9 fraud types，because other fraud type doesn't have too much examples. We also put a threshold on user behaviour, only the user has more than 10 sessions within lastest 30d we consider about active user. And have enough behaviour features for us to classify whether it is a good user or bad user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:50:45.538587Z",
     "start_time": "2019-11-08T09:50:45.529646Z"
    }
   },
   "outputs": [],
   "source": [
    "target_type = [\n",
    "    'Normal',\n",
    "    'DP Voucher',\n",
    "    'Voucher',\n",
    "    'Free Shipping',\n",
    "    'Scam - Potential Scammer',\n",
    "    'Duplicate listing',\n",
    "    'Order Brushing',\n",
    "    'Welcome Package',\n",
    "    'Promotion T&C',\n",
    "    'Coin fraud'\n",
    "]\n",
    "\n",
    "total_session_count_in_latest_30d_threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:52:50.322857Z",
     "start_time": "2019-11-08T09:50:45.544012Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17398711\n",
      "17391050\n"
     ]
    }
   ],
   "source": [
    "df = df.filter(df.total_session_count_in_last_30d >= total_session_count_in_latest_30d_threshold)\n",
    "print(df.count())\n",
    "\n",
    "column_name = 'name'\n",
    "filter_df = spark.createDataFrame(target_type, df.schema[column_name].dataType)\n",
    "df = df.join(filter_df, df[column_name] == filter_df[\"value\"])\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:54:25.807245Z",
     "start_time": "2019-11-08T09:52:50.328339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------+\n",
      "|name                    |total_count|\n",
      "+------------------------+-----------+\n",
      "|Normal                  |17288848   |\n",
      "|Voucher                 |45145      |\n",
      "|Free Shipping           |15273      |\n",
      "|Order Brushing          |13155      |\n",
      "|Duplicate listing       |10397      |\n",
      "|DP Voucher              |5471       |\n",
      "|Welcome Package         |5271       |\n",
      "|Scam - Potential Scammer|4546       |\n",
      "|Promotion T&C           |1646       |\n",
      "|Coin fraud              |1298       |\n",
      "+------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('name').agg(F.count('userid').alias('total_count'))\\\n",
    "                    .orderBy('total_count', ascending=False).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:54:25.829566Z",
     "start_time": "2019-11-08T09:54:25.811777Z"
    }
   },
   "outputs": [],
   "source": [
    "frac = 1\n",
    "fractions = {\n",
    "    'Normal':0.005 * frac,\n",
    "    'DP Voucher':1 * frac,\n",
    "    'Voucher':0.5 * frac,\n",
    "    'Free Shipping':1 * frac,\n",
    "    'Scam - Potential Scammer':1 * frac,\n",
    "    'Duplicate listing':1 * frac,\n",
    "    'Order Brushing':1 * frac,\n",
    "    'Welcome Package':1 * frac,\n",
    "    'Promotion T&C':1 * frac,\n",
    "    'Coin fraud':1 * frac\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:55:14.266742Z",
     "start_time": "2019-11-08T09:54:25.834824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------+\n",
      "|name                    |total_count|\n",
      "+------------------------+-----------+\n",
      "|Normal                  |86450      |\n",
      "|Voucher                 |22560      |\n",
      "|Free Shipping           |15273      |\n",
      "|Order Brushing          |13155      |\n",
      "|Duplicate listing       |10397      |\n",
      "|DP Voucher              |5471       |\n",
      "|Welcome Package         |5271       |\n",
      "|Scam - Potential Scammer|4546       |\n",
      "|Promotion T&C           |1646       |\n",
      "|Coin fraud              |1298       |\n",
      "+------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = df.sampleBy(\"name\", fractions=fractions, seed=0)\n",
    "dataset.groupBy('name').agg(F.count('userid').alias('total_count'))\\\n",
    "                    .orderBy('total_count', ascending=False).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:55:14.278647Z",
     "start_time": "2019-11-08T09:55:14.271928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userid: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- uid: long (nullable = true)\n",
      " |-- total_active_time_in_last_30d: double (nullable = true)\n",
      " |-- total_session_count_in_last_30d: long (nullable = true)\n",
      " |-- total_ops_count_in_last_30d: long (nullable = true)\n",
      " |-- active_days_in_last_30d: string (nullable = true)\n",
      " |-- last_active_day: date (nullable = true)\n",
      " |-- hour_normalise: string (nullable = true)\n",
      " |-- action_normalise: string (nullable = true)\n",
      " |-- op_interval: string (nullable = true)\n",
      " |-- markov: string (nullable = true)\n",
      " |-- grass_region: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may takes a lot of time, but still fast than toPandas()\n",
    "dataset.coalesce(500).write.mode('overwrite').parquet('user-behaviour-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning \n",
    "\n",
    "## preprocess dataset\n",
    "\n",
    "We use all 3 types of features. Since markov and action, active hour feature are normalised before, we just need to do column wise normalise on the other 4 columns. We use `RobustScaler` to avoid exetrme values. In total, we have 2108 dimentional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hadoop fs -get /user/ming.liu/user-behaviour-dataset ./data\n",
    "# !rm -rf ./data/user-behaviour-dataset/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:55:14.613593Z",
     "start_time": "2019-11-08T09:55:14.282971Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:55:27.653413Z",
     "start_time": "2019-11-08T09:55:14.617685Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset_pd = dataset.toPandas()\n",
    "dataset_pd =pd.read_parquet(\"./data/user-behaviour-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:56:57.847295Z",
     "start_time": "2019-11-08T09:55:43.058650Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_pd['hour_normalise'] = dataset_pd['hour_normalise'].apply(lambda x: json.loads(x))\n",
    "dataset_pd['action_normalise'] = dataset_pd['action_normalise'].apply(lambda x: json.loads(x))\n",
    "dataset_pd['op_interval'] = dataset_pd['op_interval'].apply(lambda x: json.loads(x))\n",
    "dataset_pd['markov'] = dataset_pd['markov'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:57:16.615109Z",
     "start_time": "2019-11-08T09:56:57.851269Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/ming.liu/miniconda3/envs/spy/lib/python2.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "raw_interval_data = dataset_pd.as_matrix(columns=['op_interval'])\n",
    "interval_data = [[] for _ in range(raw_interval_data.shape[0])]\n",
    "for i, d in enumerate(interval_data):\n",
    "    for c in raw_interval_data[i]:\n",
    "        interval_data[i].extend(c)\n",
    "interval_data = np.array(interval_data, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:57:17.161385Z",
     "start_time": "2019-11-08T09:57:16.618849Z"
    }
   },
   "outputs": [],
   "source": [
    "interval_data_max = np.max(interval_data, axis=1).reshape(raw_interval_data.shape[0], 1)\n",
    "row_sums = interval_data.sum(axis=1)\n",
    "interval_data_normalise = interval_data / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:57:17.224392Z",
     "start_time": "2019-11-08T09:57:17.164857Z"
    }
   },
   "outputs": [],
   "source": [
    "# column wise normalise \n",
    "x = dataset_pd[[\n",
    "    'total_active_time_in_last_30d', \n",
    "    'total_session_count_in_last_30d', \n",
    "    'total_ops_count_in_last_30d']].values.astype(float)\n",
    "\n",
    "x = np.concatenate((interval_data_max, x), axis = 1)\n",
    "\n",
    "#scaler = preprocessing.MinMaxScaler()\n",
    "scaler = preprocessing.RobustScaler() \n",
    "x_scaled = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:57:17.289794Z",
     "start_time": "2019-11-08T09:57:17.229529Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/ming.liu/miniconda3/envs/spy/lib/python2.7/site-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/ldap_home/ming.liu/miniconda3/envs/spy/lib/python2.7/site-packages/ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "raw_data = dataset_pd.as_matrix(columns=[\n",
    "#     'total_active_time_in_last_30d',\n",
    "#     'total_session_count_in_last_30d',\n",
    "#     'total_ops_count_in_last_30d',\n",
    "    'hour_normalise',\n",
    "    'action_normalise',\n",
    "#     'op_interval',\n",
    "    'markov'\n",
    "])\n",
    "label = dataset_pd.as_matrix(columns=['name']).reshape(len(dataset_pd),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:57:20.484838Z",
     "start_time": "2019-11-08T09:57:17.294707Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [[] for _ in range(raw_data.shape[0])]\n",
    "for i, d in enumerate(data):\n",
    "    for c in raw_data[i]:\n",
    "        try:\n",
    "            data[i].extend(c)\n",
    "        except:\n",
    "            data[i].append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:57:32.128778Z",
     "start_time": "2019-11-08T09:57:20.489397Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.concatenate((data, x_scaled, interval_data_normalise), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:57:33.124818Z",
     "start_time": "2019-11-08T09:57:32.135379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166065, 2108)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:57:33.239473Z",
     "start_time": "2019-11-08T09:57:33.128172Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, l in enumerate(label):\n",
    "    label[i] = target_type.index(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:57:33.251999Z",
     "start_time": "2019-11-08T09:57:33.243203Z"
    }
   },
   "outputs": [],
   "source": [
    "label = np.array(label, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM \n",
    "\n",
    "- **Results** SVM give us baseline, 0.82 - 0.83 accuracy on 10 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:59:07.076996Z",
     "start_time": "2019-11-08T09:59:07.023172Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T09:59:08.175436Z",
     "start_time": "2019-11-08T09:59:07.291698Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data,\n",
    "    label, \n",
    "    test_size=0.2, \n",
    "    random_state=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:13:20.487154Z",
     "start_time": "2019-11-08T09:59:08.516373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/ming.liu/miniconda3/envs/spy/lib/python2.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8257007798151327"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = OneVsRestClassifier(LinearSVC(random_state=0, verbose=1))\n",
    "classifier.fit(X_train, y_train)\n",
    "y_predict = classifier.predict(X_test)\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:13:20.589604Z",
     "start_time": "2019-11-08T10:13:20.496358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Normal       0.87      0.96      0.91     17395\n",
      "              DP Voucher       0.82      0.84      0.83      1166\n",
      "                 Voucher       0.69      0.66      0.67      4428\n",
      "           Free Shipping       0.68      0.50      0.57      3022\n",
      "Scam - Potential Scammer       0.70      0.51      0.59       896\n",
      "       Duplicate listing       0.79      0.85      0.82      2039\n",
      "          Order Brushing       0.89      0.85      0.87      2670\n",
      "         Welcome Package       0.86      0.74      0.79      1008\n",
      "           Promotion T&C       0.72      0.37      0.49       340\n",
      "              Coin fraud       0.70      0.08      0.15       249\n",
      "\n",
      "               micro avg       0.83      0.83      0.83     33213\n",
      "               macro avg       0.77      0.63      0.67     33213\n",
      "            weighted avg       0.82      0.83      0.82     33213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        list(y_test), \n",
    "        list(y_predict), \n",
    "        target_names = target_type\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive neural network\n",
    "\n",
    "- **Results** Simple neural network give not bad performance. 0.85 accuracy on 10 categories. Add more layers almost gives similar performance as just one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:13:43.524863Z",
     "start_time": "2019-11-08T10:13:35.023370Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf 2.0\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:20:48.460761Z",
     "start_time": "2019-11-08T10:20:48.320993Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(len(target_type), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:24:29.083818Z",
     "start_time": "2019-11-08T10:20:48.735759Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132852 samples, validate on 33213 samples\n",
      "Epoch 1/20\n",
      "132852/132852 [==============================] - 12s 88us/sample - loss: 0.9142 - accuracy: 0.8003 - val_loss: 0.7663 - val_accuracy: 0.8286\n",
      "Epoch 2/20\n",
      "132852/132852 [==============================] - 11s 84us/sample - loss: 0.7590 - accuracy: 0.8305 - val_loss: 0.7483 - val_accuracy: 0.8408\n",
      "Epoch 3/20\n",
      "132852/132852 [==============================] - 11s 83us/sample - loss: 0.7884 - accuracy: 0.8399 - val_loss: 0.6600 - val_accuracy: 0.8439\n",
      "Epoch 4/20\n",
      "132852/132852 [==============================] - 11s 82us/sample - loss: 0.7790 - accuracy: 0.8464 - val_loss: 0.6819 - val_accuracy: 0.8466\n",
      "Epoch 5/20\n",
      "132852/132852 [==============================] - 11s 82us/sample - loss: 0.7371 - accuracy: 0.8504 - val_loss: 0.8525 - val_accuracy: 0.8459\n",
      "Epoch 6/20\n",
      "132852/132852 [==============================] - 11s 83us/sample - loss: 0.7439 - accuracy: 0.8543 - val_loss: 0.6757 - val_accuracy: 0.8479\n",
      "Epoch 7/20\n",
      "132852/132852 [==============================] - 11s 83us/sample - loss: 0.7046 - accuracy: 0.8570 - val_loss: 0.6402 - val_accuracy: 0.8517\n",
      "Epoch 8/20\n",
      "132852/132852 [==============================] - 11s 83us/sample - loss: 0.7680 - accuracy: 0.8604 - val_loss: 1.5346 - val_accuracy: 0.8512\n",
      "Epoch 9/20\n",
      "132852/132852 [==============================] - 11s 81us/sample - loss: 0.7497 - accuracy: 0.8629 - val_loss: 0.7344 - val_accuracy: 0.8510\n",
      "Epoch 10/20\n",
      "132852/132852 [==============================] - 11s 82us/sample - loss: 0.8042 - accuracy: 0.8639 - val_loss: 0.6968 - val_accuracy: 0.8507\n",
      "Epoch 11/20\n",
      "132852/132852 [==============================] - 11s 82us/sample - loss: 0.7220 - accuracy: 0.8669 - val_loss: 0.8301 - val_accuracy: 0.8526\n",
      "Epoch 12/20\n",
      "132852/132852 [==============================] - 11s 82us/sample - loss: 0.6905 - accuracy: 0.8690 - val_loss: 0.8147 - val_accuracy: 0.8519\n",
      "Epoch 13/20\n",
      "132852/132852 [==============================] - 11s 81us/sample - loss: 0.7845 - accuracy: 0.8705 - val_loss: 0.8497 - val_accuracy: 0.8546\n",
      "Epoch 14/20\n",
      "132852/132852 [==============================] - 11s 82us/sample - loss: 0.9085 - accuracy: 0.8724 - val_loss: 0.9754 - val_accuracy: 0.8522\n",
      "Epoch 15/20\n",
      "132852/132852 [==============================] - 11s 80us/sample - loss: 0.8588 - accuracy: 0.8740 - val_loss: 1.1737 - val_accuracy: 0.8550\n",
      "Epoch 16/20\n",
      "132852/132852 [==============================] - 11s 82us/sample - loss: 0.7437 - accuracy: 0.8762 - val_loss: 1.3117 - val_accuracy: 0.8495\n",
      "Epoch 17/20\n",
      "132852/132852 [==============================] - 11s 82us/sample - loss: 0.8144 - accuracy: 0.8767 - val_loss: 0.9518 - val_accuracy: 0.8519\n",
      "Epoch 18/20\n",
      "132852/132852 [==============================] - 11s 82us/sample - loss: 0.8115 - accuracy: 0.8791 - val_loss: 0.8432 - val_accuracy: 0.8532\n",
      "Epoch 19/20\n",
      "132852/132852 [==============================] - 11s 83us/sample - loss: 0.7801 - accuracy: 0.8798 - val_loss: 0.8975 - val_accuracy: 0.8519\n",
      "Epoch 20/20\n",
      "132852/132852 [==============================] - 11s 82us/sample - loss: 0.7118 - accuracy: 0.8808 - val_loss: 0.7771 - val_accuracy: 0.8514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc9147775d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    use_multiprocessing=True, \n",
    "    validation_data=(X_test,  y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:24:30.696485Z",
     "start_time": "2019-11-08T10:24:29.086981Z"
    }
   },
   "outputs": [],
   "source": [
    "y_predict = np.argmax(model.predict(X_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:24:30.790474Z",
     "start_time": "2019-11-08T10:24:30.701360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Normal       0.91      0.95      0.93     17395\n",
      "              DP Voucher       0.87      0.85      0.86      1166\n",
      "                 Voucher       0.71      0.71      0.71      4428\n",
      "           Free Shipping       0.69      0.61      0.65      3022\n",
      "Scam - Potential Scammer       0.70      0.63      0.66       896\n",
      "       Duplicate listing       0.84      0.87      0.85      2039\n",
      "          Order Brushing       0.90      0.87      0.89      2670\n",
      "         Welcome Package       0.86      0.77      0.81      1008\n",
      "           Promotion T&C       0.78      0.54      0.64       340\n",
      "              Coin fraud       0.57      0.22      0.32       249\n",
      "\n",
      "               micro avg       0.85      0.85      0.85     33213\n",
      "               macro avg       0.78      0.70      0.73     33213\n",
      "            weighted avg       0.85      0.85      0.85     33213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        list(y_test), \n",
    "        list(y_predict), \n",
    "        target_names = target_type\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightGBM\n",
    "\n",
    "- **Results** GBDT gives highest performance, around 0.87, but also takes many time to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install setuptools wheel numpy scipy scikit-learn -U\n",
    "# !{sys.executable} -m pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = 0.87\n",
    "# classifier = LGBMClassifier(\n",
    "#     num_leaves=31, \n",
    "#     max_depth=-1, \n",
    "#     learning_rate=0.1, \n",
    "#     n_estimators=300, \n",
    "#     n_jobs=50,\n",
    "#     silent=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:26:44.839099Z",
     "start_time": "2019-11-08T10:26:44.813482Z"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:29:59.812417Z",
     "start_time": "2019-11-08T10:26:45.164847Z"
    }
   },
   "outputs": [],
   "source": [
    "# all default params, just add more epoch 100 -> 300\n",
    "\n",
    "classifier = LGBMClassifier(n_estimators=300)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_predict = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:29:59.869396Z",
     "start_time": "2019-11-08T10:29:59.817015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Normal       0.92      0.97      0.94     17395\n",
      "              DP Voucher       0.89      0.86      0.88      1166\n",
      "                 Voucher       0.73      0.75      0.74      4428\n",
      "           Free Shipping       0.72      0.63      0.67      3022\n",
      "Scam - Potential Scammer       0.79      0.66      0.72       896\n",
      "       Duplicate listing       0.86      0.89      0.87      2039\n",
      "          Order Brushing       0.92      0.88      0.90      2670\n",
      "         Welcome Package       0.90      0.80      0.85      1008\n",
      "           Promotion T&C       0.85      0.59      0.69       340\n",
      "              Coin fraud       0.51      0.22      0.31       249\n",
      "\n",
      "               micro avg       0.87      0.87      0.87     33213\n",
      "               macro avg       0.81      0.72      0.76     33213\n",
      "            weighted avg       0.86      0.87      0.86     33213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        list(y_test), \n",
    "        list(y_predict), \n",
    "        target_names = target_type\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原生的 lightgbm 完全玩不来。。。 学了一天调了一天还没有上面封装的 default 来的高。。。 但是，用它提供个的 Dataset 来存放数据只会使用很少的内存。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
    "# test_data = lgb.Dataset(X_test, label=y_test, free_raw_data=False)\n",
    "\n",
    "# parameters = {\n",
    "#     'objective': 'multiclass',\n",
    "#     'num_classes': len(target_type),\n",
    "#     'metric': 'softmax',\n",
    "#     'is_unbalance': 'true',\n",
    "#     'boosting': 'gbdt',\n",
    "#     'num_leaves': 31,\n",
    "# #     'feature_fraction': 0.8,\n",
    "# #     'bagging_fraction': 0.5,\n",
    "# #     'bagging_freq': 20,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'max_bin': 255,\n",
    "#     'verbose': 1\n",
    "# }\n",
    "\n",
    "# gbm = lgb.train(\n",
    "#     parameters,\n",
    "#     train_data,\n",
    "# #     valid_sets=[test_data],\n",
    "#     num_boost_round=300,\n",
    "# #     verbose_eval=5,\n",
    "# #     early_stopping_rounds=30,\n",
    "# #     init_model='lgb_model.txt'\n",
    "# )\n",
    "\n",
    "# y_predict = np.argmax(gbm.predict(X_test), axis = 1)\n",
    "\n",
    "# gbm.save_model('lgb_model.txt')\n",
    "# # bst = lgb.Booster(model_file='model.txt')\n",
    "\n",
    "# print(\n",
    "#     classification_report(\n",
    "#         list(y_test), \n",
    "#         list(y_predict), \n",
    "#         target_names = target_type\n",
    "#     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raise Threshold To See What Happens\n",
    "\n",
    "\n",
    "## Only examine predictions with high confidence\n",
    "- **Conclusion** very promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:29:59.881954Z",
     "start_time": "2019-11-08T10:29:59.872937Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_result(predictions, labels, confidence_threshold):\n",
    "    f_prediction = []\n",
    "    f_label = []\n",
    "    for i, p in enumerate(predictions):\n",
    "        if max(p) >= confidence_threshold:\n",
    "            f_prediction.append(p)\n",
    "            f_label.append(labels[i])\n",
    "    \n",
    "    print('%.3f are left' % (float(len(f_label)) / len(labels)))\n",
    "    print('=' * 20)\n",
    "    \n",
    "    y_predict = np.argmax(f_prediction, axis = 1)\n",
    "    print(\n",
    "    classification_report(\n",
    "        list(f_label), \n",
    "        list(y_predict), \n",
    "        target_names = target_type\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:32:02.537724Z",
     "start_time": "2019-11-08T10:32:01.982165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770 are left\n",
      "====================\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Normal       0.96      0.99      0.98     15771\n",
      "              DP Voucher       0.95      0.96      0.95       973\n",
      "                 Voucher       0.88      0.84      0.86      2270\n",
      "           Free Shipping       0.89      0.77      0.83      1215\n",
      "Scam - Potential Scammer       0.95      0.80      0.87       476\n",
      "       Duplicate listing       0.93      0.98      0.96      1536\n",
      "          Order Brushing       0.98      0.97      0.97      2216\n",
      "         Welcome Package       0.96      0.88      0.92       815\n",
      "           Promotion T&C       0.92      0.75      0.82       210\n",
      "              Coin fraud       0.76      0.34      0.47        82\n",
      "\n",
      "               micro avg       0.95      0.95      0.95     25564\n",
      "               macro avg       0.92      0.83      0.86     25564\n",
      "            weighted avg       0.95      0.95      0.95     25564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_result(classifier.predict_proba(X_test), y_test, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only allow more active user to be trained and tested\n",
    "\n",
    "- **Conclusion** make more restrict threshold on user activeness doesn't give too much accuracy improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:26:36.034066Z",
     "start_time": "2019-11-08T10:26:36.027940Z"
    }
   },
   "outputs": [],
   "source": [
    "min_session = 30     # only consider user with more than 30 sessions within last 30 days\n",
    "min_time = 30 * 60   # or consider user spent more than 30 min within last 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:32:24.246879Z",
     "start_time": "2019-11-08T10:32:24.172018Z"
    }
   },
   "outputs": [],
   "source": [
    "f1 = dataset_pd['total_active_time_in_last_30d'] >= min_time\n",
    "f2 = dataset_pd['total_session_count_in_last_30d'] >= min_session\n",
    "dataset_pd_restrict = dataset_pd[f1 | f2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:32:25.985518Z",
     "start_time": "2019-11-08T10:32:25.978992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.862 data left after applied filter\n"
     ]
    }
   ],
   "source": [
    "print('%.3f data left after applied filter' % (len(dataset_pd_restrict) / float(len(dataset_pd))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:32:30.013371Z",
     "start_time": "2019-11-08T10:32:29.933965Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(dataset_pd):\n",
    "    raw_interval_data = dataset_pd.as_matrix(columns=['op_interval'])\n",
    "    interval_data = [[] for _ in range(raw_interval_data.shape[0])]\n",
    "    for i, d in enumerate(interval_data):\n",
    "        for c in raw_interval_data[i]:\n",
    "            interval_data[i].extend(c)\n",
    "    interval_data = np.array(interval_data, dtype='float')\n",
    "    interval_data_max = np.max(interval_data, axis=1).reshape(raw_interval_data.shape[0], 1)\n",
    "\n",
    "    row_sums = interval_data.sum(axis=1)\n",
    "    interval_data_normalise = interval_data / row_sums[:, np.newaxis]\n",
    "\n",
    "    x = dataset_pd[[\n",
    "        'total_active_time_in_last_30d', \n",
    "        'total_session_count_in_last_30d', \n",
    "        'total_ops_count_in_last_30d']].values.astype(float)\n",
    "\n",
    "    x = np.concatenate((interval_data_max, x), axis = 1)\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    \n",
    "    raw_data = dataset_pd.as_matrix(columns=[\n",
    "    #     'total_active_time_in_last_30d',\n",
    "    #     'total_session_count_in_last_30d',\n",
    "    #     'total_ops_count_in_last_30d',\n",
    "        'hour_normalise',\n",
    "        'action_normalise',\n",
    "    #     'op_interval',\n",
    "        'markov'\n",
    "    ])\n",
    "    label = dataset_pd.as_matrix(columns=['name']).reshape(len(dataset_pd),)\n",
    "    \n",
    "    data = [[] for _ in range(raw_data.shape[0])]\n",
    "    for i, d in enumerate(data):\n",
    "        for c in raw_data[i]:\n",
    "            try:\n",
    "                data[i].extend(c)\n",
    "            except:\n",
    "                data[i].append(c)\n",
    "                \n",
    "    data = np.concatenate((data, x_scaled, interval_data_normalise), axis = 1)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    for i, l in enumerate(label):\n",
    "        label[i] = target_type.index(l)\n",
    "    label = np.array(label, dtype=int)\n",
    "    \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:33:07.172713Z",
     "start_time": "2019-11-08T10:32:35.211678Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/ming.liu/miniconda3/envs/spy/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/ldap_home/ming.liu/miniconda3/envs/spy/lib/python2.7/site-packages/ipykernel_launcher.py:30: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/ldap_home/ming.liu/miniconda3/envs/spy/lib/python2.7/site-packages/ipykernel_launcher.py:32: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "data, label = data_preprocessing(dataset_pd_restrict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:33:08.091048Z",
     "start_time": "2019-11-08T10:33:07.177376Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data,\n",
    "    label, \n",
    "    test_size=0.2, \n",
    "    random_state=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:36:10.075372Z",
     "start_time": "2019-11-08T10:33:08.095507Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = LGBMClassifier(n_estimators=300)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_predict = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:36:10.122840Z",
     "start_time": "2019-11-08T10:36:10.079584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Normal       0.92      0.97      0.95     16499\n",
      "              DP Voucher       0.86      0.82      0.84       608\n",
      "                 Voucher       0.74      0.74      0.74      3998\n",
      "           Free Shipping       0.70      0.64      0.67      2595\n",
      "Scam - Potential Scammer       0.79      0.63      0.70       736\n",
      "       Duplicate listing       0.84      0.83      0.84      1143\n",
      "          Order Brushing       0.93      0.89      0.91      1868\n",
      "         Welcome Package       0.90      0.77      0.83       792\n",
      "           Promotion T&C       0.88      0.49      0.63       199\n",
      "              Coin fraud       0.69      0.18      0.28       195\n",
      "\n",
      "               micro avg       0.87      0.87      0.87     28633\n",
      "               macro avg       0.83      0.70      0.74     28633\n",
      "            weighted avg       0.87      0.87      0.87     28633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        list(y_test), \n",
    "        list(y_predict), \n",
    "        target_names = target_type\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T10:37:14.076687Z",
     "start_time": "2019-11-08T10:37:13.544848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.775 are left\n",
      "====================\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Normal       0.97      0.99      0.98     15038\n",
      "              DP Voucher       0.92      0.91      0.92       494\n",
      "                 Voucher       0.88      0.84      0.86      2010\n",
      "           Free Shipping       0.90      0.76      0.83      1073\n",
      "Scam - Potential Scammer       0.94      0.80      0.86       366\n",
      "       Duplicate listing       0.94      0.97      0.95       829\n",
      "          Order Brushing       0.98      0.98      0.98      1567\n",
      "         Welcome Package       0.95      0.86      0.90       619\n",
      "           Promotion T&C       0.94      0.70      0.80       112\n",
      "              Coin fraud       0.78      0.38      0.51        77\n",
      "\n",
      "               micro avg       0.95      0.95      0.95     22185\n",
      "               macro avg       0.92      0.82      0.86     22185\n",
      "            weighted avg       0.95      0.95      0.95     22185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_result(classifier.predict_proba(X_test), y_test, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python 2.7",
   "language": "python",
   "name": "spy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
